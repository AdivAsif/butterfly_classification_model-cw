{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport shutil\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\nimport csv\n\n# puts contents from train.txt in a pandas dataframe to sort the training and validation datasets to folders from 0-22\ndata = pd.read_csv('../input/uos-com2028/train.txt', delimiter = \" \", header=None)\n# 80% training, 20% validation\nsplit_size = int(len(data.index)*0.2)\n\n# creates folders for validation and training\nif not os.path.exists(\"/kaggle/working/validation/\"):\n    os.mkdir(\"/kaggle/working/validation/\")\nif not os.path.exists(\"/kaggle/working/training\"):\n    os.mkdir(\"/kaggle/working/training\")\n\n# sorts images into validation and training based off the train.txt dataframe to sort everything\nfor path, label in data.values[:split_size]:\n    validation_path = \"/kaggle/working/validation/\" + str(label)\n    if not os.path.exists(validation_path):\n        os.mkdir(validation_path)\n    shutil.copyfile(\"../input/uos-com2028/train/\" + path, validation_path + path.strip(\"train\"))\nfor path, label in data.values[split_size:]:\n    training_path = \"/kaggle/working/training/\" + str(label)\n    if not os.path.exists(training_path):\n        os.mkdir(training_path)\n    shutil.copyfile(\"../input/uos-com2028/train/\" + path, training_path + path.strip(\"train\"))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"image_h = 150\nimage_w = 150\nbatch_size = 32\n\n# makes this generalise images (data augmentation); applies this to all of the datasets (training, validation, testing)\ndatagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    brightness_range=[0.8,1.2],\n    horizontal_flip=True\n)\n\n# image data generator for training dataset, resizes everything to (150, 150), makes it have a class size of 23, and shuffles images\ntrain_generator = datagen.flow_from_directory(\n    \"/kaggle/working/training\",\n    target_size=(image_h, image_w),\n    batch_size=batch_size,\n    classes= [str(i) for i in range(0,23)],\n    class_mode=\"categorical\",\n    shuffle=True\n)\n\n# same as the train_generator\nvalidation_generator = datagen.flow_from_directory(\n    \"/kaggle/working/validation\",\n    target_size=(image_h, image_w),\n    batch_size=batch_size,\n    classes= [str(i) for i in range(0,23)],\n    class_mode=\"categorical\",\n    shuffle=True\n)\n\n# slightly different to the other generators, shuffle is true by default, for a proper csv file it must be false\ntest_generator = datagen.flow_from_directory(\n    '/kaggle/input/uos-com2028/test',\n    target_size=(image_h, image_w),\n    batch_size=32,\n    shuffle=False,\n    class_mode=None\n)\n\n# get lengths of the datasets for steps\ntrain_n = len(train_generator.filenames)\nval_n = len(validation_generator.filenames)\ntest_n = len(test_generator.filenames)\n# (150, 150, 3) - 150 for shorter epoch times\ninput_shape = (image_h, image_w, 3)","metadata":{"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Found 8216 images belonging to 23 classes.\nFound 2054 images belonging to 23 classes.\nFound 15009 images belonging to 1 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"base_model = keras.applications.Xception(\n    weights='imagenet',  # Load weights pre-trained on ImageNet.\n    input_shape=input_shape,\n    include_top=False)  # Do not include the ImageNet classifier at the top.\n\n# high patience, and monitoring val_loss\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n\nbase_model.trainable = False\n\ninputs = keras.Input(shape=input_shape)\n# We make sure that the base_model is running in inference mode here,\n# by passing `training=False`. This is important for fine-tuning.\nx = base_model(inputs, training=False)\n# Convert features of shape `base_model.output_shape[1:]` to vectors\nx = keras.layers.GlobalAveragePooling2D()(x)\n# A Dense classifier with a single unit (Categorical classification)\n\noutputs = keras.layers.Dense(23)(x)\nmodel = keras.Model(inputs, outputs)\n\n# high learning rate for the base model\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=0.1, epsilon=0.1),\n              loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=[keras.metrics.CategoricalAccuracy()])\nmodel.fit(\n    train_generator,\n    steps_per_epoch=train_n // batch_size,\n    epochs=50,\n    verbose=1,\n    validation_data=validation_generator,\n    validation_steps=val_n // batch_size,\n    callbacks=[callback]\n)\n\nbase_model.trainable = True\n\n# low learning rate for the new model\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4, epsilon=1e-4),\n              loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=[keras.metrics.CategoricalAccuracy()])\nmodel.fit(\n    train_generator,\n    steps_per_epoch=train_n // batch_size,\n    epochs=50,\n    verbose=1,\n    validation_data=validation_generator,\n    validation_steps=val_n // batch_size,\n    callbacks=[callback]\n)\n\n# base model average epoch times: 97s\n# new model average epoch times: 113s","metadata":{"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Epoch 1/50\n256/256 [==============================] - 99s 380ms/step - loss: 2.1438 - categorical_accuracy: 0.4472 - val_loss: 2.2037 - val_categorical_accuracy: 0.5425\nEpoch 2/50\n256/256 [==============================] - 96s 375ms/step - loss: 2.2605 - categorical_accuracy: 0.5610 - val_loss: 3.0177 - val_categorical_accuracy: 0.5688\nEpoch 3/50\n256/256 [==============================] - 97s 379ms/step - loss: 2.5938 - categorical_accuracy: 0.6049 - val_loss: 3.4189 - val_categorical_accuracy: 0.5654\nEpoch 4/50\n256/256 [==============================] - 97s 378ms/step - loss: 2.4561 - categorical_accuracy: 0.6376 - val_loss: 3.8053 - val_categorical_accuracy: 0.5664\nEpoch 5/50\n256/256 [==============================] - 97s 379ms/step - loss: 2.6546 - categorical_accuracy: 0.6397 - val_loss: 3.8718 - val_categorical_accuracy: 0.5469\nEpoch 6/50\n256/256 [==============================] - 97s 378ms/step - loss: 2.9448 - categorical_accuracy: 0.6400 - val_loss: 4.2855 - val_categorical_accuracy: 0.5796\nEpoch 7/50\n256/256 [==============================] - 97s 380ms/step - loss: 2.7726 - categorical_accuracy: 0.6544 - val_loss: 3.6807 - val_categorical_accuracy: 0.6113\nEpoch 1/50\n256/256 [==============================] - 119s 449ms/step - loss: 1.1164 - categorical_accuracy: 0.6824 - val_loss: 0.7601 - val_categorical_accuracy: 0.7739\nEpoch 2/50\n256/256 [==============================] - 115s 450ms/step - loss: 0.5011 - categorical_accuracy: 0.8424 - val_loss: 0.6029 - val_categorical_accuracy: 0.8140\nEpoch 3/50\n256/256 [==============================] - 115s 447ms/step - loss: 0.3150 - categorical_accuracy: 0.8958 - val_loss: 0.5342 - val_categorical_accuracy: 0.8315\nEpoch 4/50\n256/256 [==============================] - 115s 448ms/step - loss: 0.2353 - categorical_accuracy: 0.9258 - val_loss: 0.4873 - val_categorical_accuracy: 0.8584\nEpoch 5/50\n256/256 [==============================] - 113s 440ms/step - loss: 0.1880 - categorical_accuracy: 0.9372 - val_loss: 0.4213 - val_categorical_accuracy: 0.8745\nEpoch 6/50\n256/256 [==============================] - 113s 439ms/step - loss: 0.1309 - categorical_accuracy: 0.9567 - val_loss: 0.5065 - val_categorical_accuracy: 0.8711\nEpoch 7/50\n256/256 [==============================] - 111s 434ms/step - loss: 0.1248 - categorical_accuracy: 0.9587 - val_loss: 0.5586 - val_categorical_accuracy: 0.8447\nEpoch 8/50\n256/256 [==============================] - 111s 435ms/step - loss: 0.0978 - categorical_accuracy: 0.9702 - val_loss: 0.4918 - val_categorical_accuracy: 0.8735\nEpoch 9/50\n256/256 [==============================] - 112s 437ms/step - loss: 0.0966 - categorical_accuracy: 0.9667 - val_loss: 0.4333 - val_categorical_accuracy: 0.8804\nEpoch 10/50\n256/256 [==============================] - 111s 434ms/step - loss: 0.0711 - categorical_accuracy: 0.9773 - val_loss: 0.5237 - val_categorical_accuracy: 0.8755\nEpoch 11/50\n256/256 [==============================] - 111s 435ms/step - loss: 0.0856 - categorical_accuracy: 0.9743 - val_loss: 0.4413 - val_categorical_accuracy: 0.8745\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f97b831ba10>"},"metadata":{}}]},{"cell_type":"code","source":"predictions = model.predict(test_generator, steps=test_n // 32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"row_list = [[\"id\", \"label\"]]\nfor x in range(len(predictions)):\n    row_list.append([test_generator.filenames[x].strip('test/.jpg'), np.argmax(predictions[x])])\n\nwith open(\"6586214.csv\", \"w\", newline = \"\") as file:\n    writer = csv.writer(file)\n    writer.writerows(row_list)\n\nrow_list = None","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}